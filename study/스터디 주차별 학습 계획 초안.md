### 📚 0. 들어가기 전: 핵심 도구(Tool) 설명서
* **ROS 2 (Robot Operating System 2):** 로봇의 **'신경망'**. 카메라, 모터, AI 알고리즘 같은 각기 다른 부품(세포)들이 서로 대화할 수 있게 해주는 통신 시스템
* **Gazebo (가제보):** 로봇을 위한 **'매트릭스(가상 현실)'**. 실제 로봇 없이도 물리 법칙(중력, 마찰력 등)이 적용된 가상 공간에서 로봇을 실험할 수 있게 해주는 시뮬레이터
* **Nav2 (Navigation 2):** 로봇의 **'내비게이션 + 운전기사'**. 지도에서 길을 찾고(경로 생성), 장애물을 피해 핸들을 돌리는(제어) 역할을 하는 ROS 2의 패키지
* **SLAM (ORB-SLAM3 / RTAB-Map):** 로봇의 **'눈과 기억'**. 카메라 영상을 보고 "여기가 어디지?"를 판단하고 지도를 그림.

---

### 🗓️ Phase 1. 눈을 뜨다: Vision AI와 공간의 이해 (1~4주 차)

**목표:** 카메라 영상 하나로 3D 공간을 만들어내는 원리를 익힙니다. (하드웨어 없이 노트북만 사용)

#### **1주 차: 사진으로 3D 동상 만들기 (Offline Reconstruction)**

* **내용:** 스마트폰으로 책상 위 물체를 360도 찍고, 컴퓨터로 3D 모델로 변환해봅니다.
* **사용 툴:**
    - **COLMAP:** 사진 여러 장의 공통점을 찾아 3D 점(Point Cloud)으로 바꿔주는 프로그램.

* **필수 기초 지식:**
    - **[수학] 선형대수학 (행렬):** 이미지는 결국 숫자로 된 행렬입니다. 사진을 회전시키거나 늘리는 계산에 행렬 곱셈이 쓰입니다.
    - **[수학] 기하학 (Epipolar Geometry):** 왼쪽 눈과 오른쪽 눈이 다르게 보이는 원리로 거리를 재는 방법입니다.



#### **2주 차: 카메라 렌즈 깎기 (Camera Calibration)**

* **내용:** 모든 카메라는 오목/볼록 렌즈 때문에 세상이 휘어져 보입니다. 이걸 펴주는 작업을 합니다. Vision 엔지니어의 기본 소양입니다.
* **사용 툴:**
    - **OpenCV:** 영상 처리를 위한 가장 유명한 파이썬 라이브러리.
    - **Checkerboard:** 흑백 체크무늬 종이 (이걸 찍어서 왜곡을 계산함).

* **필수 기초 지식:**
    - **[물리] 광학 (Optics):** 초점 거리(Focal length), 렌즈 왜곡(Distortion)의 원리.
    - **[수학] 좌표계 변환:** 3D 세상의 좌표 $(X, Y, Z)$가 2D 이미지 좌표 $(u, v)$로 찍히는 공식.



#### **3주 차: ROS 2 환경 구축 및 실시간 영상 수신**

* **내용:** 리눅스(Ubuntu)에 ROS 2를 설치하고, 웹캠 영상을 ROS 시스템 안으로 가져옵니다.
* **사용 툴:**
    - **Docker:** 내 컴퓨터 환경을 더럽히지 않고 깔끔하게 개발 환경을 만드는 '컨테이너' 도구.
    - **ROS 2 (Humble):** 미들웨어 설치.


* **필수 기초 지식:**
    - **[CS] 리눅스 터미널:** `cd`, `ls`, `source` 같은 기본 명령어.
    - **[CS] Pub/Sub 패턴:** "방송국(Publisher)이 영상을 송출하면, 시청자(Subscriber)가 받는다"는 통신 개념.



#### **4주 차: 웹캠 하나로 내 위치 추적하기 (Visual Odometry)**

* **내용:** 노트북을 들고 움직이면, 화면 속에서 내 이동 경로(선)가 그려지게 합니다.
* **사용 툴:**
    - **ORB-SLAM3:** 카메라 기반 SLAM의 끝판왕 오픈소스.


* **필수 기초 지식:**
    - **[수학] 최적화 (Optimization):** 계산된 위치와 실제 위치의 오차를 최소한으로 줄이는 수학적 기법 (Bundle Adjustment).



---

### 🗓️ Phase 2. 길을 찾다: 가상 공간과 자율 주행 (5~8주 차)

**목표:** 가상 세계(Gazebo)에 로봇을 소환하고, 스스로 지도를 그려 목적지까지 가게 합니다.

#### **5주 차: 가상 세계(Matrix) 구축하기**

* **내용:** Gazebo 안에 가상의 집(House)을 짓고, 바퀴 달린 로봇을 소환합니다.
* **사용 툴:**
    - **Gazebo Simulator:** 물리 엔진 시뮬레이터.
    - **URDF:** "바퀴는 몸통에서 10cm 옆에 달려있다"라고 로봇의 뼈대를 설명하는 파일 형식.


* **필수 기초 지식:**
    - **[물리] 고전 역학:** 마찰력, 중력, 관성 (가상 로봇도 바퀴가 헛돌거나 미끄러집니다).
    - **[수학] TF (Transform Tree):** 로봇 몸통 기준 좌표와 카메라 기준 좌표가 어떻게 다른지 연결하는 트리 구조.



#### **6주 차: 시각 기반 지도 작성 (Visual SLAM Mapping)**

* **내용:** 가상 로봇을 키보드로 조종하며 집안 구석구석을 보여주고, 2D 지도를 완성합니다.
* **사용 툴:**
    - **RTAB-Map:** 비전 데이터를 이용해 Nav2가 쓸 수 있는 지도를 만들어주는 패키지.


* **필수 기초 지식:**
    - **[수학] 확률 (Probability):** "이 앞이 벽일 확률이 90%다"라는 식으로 지도를 채워나가는 베이즈 필터 개념.



#### **7주 차: 내비게이션 설정 (Nav2 Basics)**

* **내용:** 지도에서 '주방'을 클릭하면 로봇이 알아서 경로를 계산하고 이동합니다.
* **사용 툴:**
    - **Nav2 (Navigation Stack):** 경로 계획 및 주행 제어.


* **필수 기초 지식:**
    - **[알고리즘] 길 찾기 (A* / Dijkstra):** 게임에서 유닛이 길을 찾을 때 쓰는 것과 똑같은 최단 경로 알고리즘.
    - **[수학] 비용 지도 (Costmap):** 장애물 주변을 '위험한 비용'으로 숫자로 표현하는 법.



#### **8주 차: 자율 주행 튜닝 (Tuning)**

* **내용:** 로봇이 문틀에 끼거나 벽에 너무 붙지 않도록 파라미터(설정값)를 조절합니다. 제일 어렵고 중요합니다.
* **필수 기초 지식:**
    -  **[제어 공학] PID 제어:** 로봇이 목표 지점에 정확히 멈추거나 부드럽게 회전하도록 모터 속도를 조절하는 미분/적분 공식.



---

### 🗓️ Phase 3. 듣고 판단하다: AI 융합 및 최종 프로젝트 (9~12주 차)

**목표:** 음성 명령을 연동하고 테슬라 FSD 기능을 흉내 내며 프로젝트를 완성합니다.

#### **9주 차: "자비스, 내 말 들려?" (Voice Command)**

* **내용:** 마이크에 대고 말한 내용을 텍스트로 변환합니다.
* **사용 툴:**
    - **OpenAI Whisper API** (또는 Google STT): 고성능 음성 인식 모델.
    - **Python:** 텍스트 처리.


* **필수 기초 지식:**
    - **[CS] API 호출:** 외부 서버(OpenAI)에 데이터를 보내고 결과를 받는 법.



#### **10주 차: 말하는 대로 움직이기 (Voice Navigation)**

* **내용:** "거실로 가"  `거실` 좌표 검색  Nav2에게 "좌표 $(3.5, 2.0)$으로 이동해" 명령 전달.
* **사용 툴:**
    - **ROS 2 Action Client:** Nav2에게 코드로 명령을 내리는 방식.


* **필수 기초 지식:**
    - **[논리] 상태 머신 (State Machine):** `대기 중`  `명령 수신`  `이동 중`  `도착` 같은 로봇의 상태 흐름도 그리기.



#### **11주 차: 테슬라 FSD 맛보기 (Deep Learning)**

* **내용:** 카메라 화면에서 바닥의 차선이나 주차 라인을 딥러닝으로 인식해 봅니다.
* **사용 툴:**
    - **YOLOv8-Seg:** 객체의 모양(영역)을 따는 Segmentation 모델.

* **필수 기초 지식:**
    - **[AI] CNN 기초:** 이미지를 학습해서 이것이 '차선'인지 '바닥'인지 구분하는 원리.



#### **12주 차: 데모 영상 제작 및 포트폴리오 정리**

* **내용:** 지금까지 만든 것을 통합하고, 시연 영상을 찍고, 깃허브에 올립니다.
* **산출물:**
    - 깃허브 리드미 (프로젝트 설명).
    - 유튜브 데모 영상 (음성 명령으로 자율 주행하는 모습).
